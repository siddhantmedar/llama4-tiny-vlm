{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05886ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af1abf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3,224,224)\n",
    "\n",
    "b,c,h,w = x.shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efe9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = 196\n",
    "patch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da6f0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 14, 224, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfolded_h = x.unfold(dimension=2, size=patch_size, step=patch_size)\n",
    "unfolded_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89246de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 14, 14, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unfolded_w = unfolded_h.unfold(dimension=3, size=patch_size, step=patch_size)\n",
    "unfolded_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f467246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels, patches vertically, patches horizontally ,height of each patch, width of each patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19cdb05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 196, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = unfolded_w.reshape(\n",
    "    b,num_patches, -1\n",
    ")  # [num_patches, channels * patch_h * patch_w] => [4, 3*4*4] = [4, 48]\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1600e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionAligment(nn.Module):\n",
    "    def __init__(self,channels,num_patches,patch_size):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = channels*patch_size*patch_size\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1,self.num_patches,self.embed_dim)*0.02)\n",
    "        self.fc = nn.Linear(self.embed_dim,self.embed_dim,bias=False)\n",
    "\n",
    "        self.vit_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "            d_model=768,\n",
    "            nhead=12,\n",
    "            dim_feedforward= 3072,\n",
    "            activation= 'gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "            ),\n",
    "            num_layers=6,\n",
    "            enable_nested_tensor=False)\n",
    "        \n",
    "        for param in self.vit_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.proj_out = nn.Linear(self.embed_dim,self.embed_dim,bias=False)\n",
    "\n",
    "    def _extract_patches(self,x,patch_size,num_patches):\n",
    "        batch = x.shape[0]\n",
    "        unfolded_h = x.unfold(dimension=2, size=patch_size, step=patch_size)\n",
    "        unfolded_w = unfolded_h.unfold(dimension=3, size=patch_size, step=patch_size)\n",
    "        patches = unfolded_w.reshape(\n",
    "            batch, num_patches, -1\n",
    "        ) # [batch, num_patches, embed_dim\n",
    "        return patches\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # x: [b,c,h,w]\n",
    "        patches = self._extract_patches(x,self.patch_size,self.num_patches)  # [batch, num_patches, embed_dim]\n",
    "        patches = self.fc(patches) + self.pos_embed  # [batch, num_patches, embed_dim]\n",
    "        patches = self.vit_encoder(patches) # [batch, num_patches, embed_dim]\n",
    "        patches = self.proj_out(patches) # [batch, num_patches, embed_dim]\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f94e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 196, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionAligment(3,196,16)\n",
    "x = torch.randn(2,3,224,224)\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "xb555gupl8m",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision encoder for LLaMA 4 vision-language alignment.\n",
    "    Uses pretrained ViT-B/16 (frozen) with a trainable MLP projector.\n",
    "    \n",
    "    Flow: Image -> Pretrained ViT -> MLP Projector -> LLM-compatible embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_embed_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained ViT-B/16 (trained on ImageNet)\n",
    "        vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Extract components\n",
    "        self.patch_embed = vit.conv_proj          # [B,3,224,224] -> [B,768,14,14]\n",
    "        self.pos_embed = vit.encoder.pos_embedding # [1, 197, 768] (includes CLS)\n",
    "        self.encoder = vit.encoder.layers          # 12 transformer blocks\n",
    "        self.norm = vit.encoder.ln                 # Final LayerNorm\n",
    "        \n",
    "        # Freeze entire vision encoder\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Trainable 2-layer MLP projector (like LLaVA)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(768 * 4, llm_embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, 224, 224]\n",
    "        \n",
    "        # Patch embedding via Conv2d\n",
    "        x = self.patch_embed(x)              # [B, 768, 14, 14]\n",
    "        x = x.flatten(2).transpose(1, 2)     # [B, 196, 768]\n",
    "        \n",
    "        # Add positional embeddings (skip CLS token position at index 0)\n",
    "        x = x + self.pos_embed[:, 1:, :]     # [B, 196, 768]\n",
    "        \n",
    "        # Transformer encoder\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)                     # [B, 196, 768]\n",
    "        \n",
    "        # Project to LLM embedding space\n",
    "        x = self.proj(x)                     # [B, 196, llm_embed_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1812c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/smedar/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330M/330M [00:04<00:00, 75.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 4,722,432 / 90,520,320 (5.2%)\n",
      "Input:  torch.Size([2, 3, 224, 224])\n",
      "Output: torch.Size([2, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test the vision encoder\n",
    "vision_encoder = VisionEncoder(llm_embed_dim=768)\n",
    "\n",
    "# Check trainable vs frozen params\n",
    "trainable = sum(p.numel() for p in vision_encoder.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in vision_encoder.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "out = vision_encoder(x)\n",
    "print(f\"Input:  {x.shape}\")\n",
    "print(f\"Output: {out.shape}\")  # [B, 196, 768] - ready for LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae542aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
