{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preparation for Vision-Language Alignment\n",
    "\n",
    "**Goal:** Create a dataset that returns `(image, tokenized_caption)` pairs for training.\n",
    "\n",
    "**Key insight:** We prepend `<image>` token to each caption. During forward pass, this single token gets **replaced** by 196 vision tokens from the ViT encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Dataset\n",
    "\n",
    "We use COCO Captions via HuggingFace `datasets` library with streaming.\n",
    "\n",
    "**Why streaming?**\n",
    "- COCO has ~118K images (~13GB)\n",
    "- Streaming loads data on-the-fly, no need to download everything upfront\n",
    "- Memory efficient for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012c9cd410a74313a60f3b322eae3761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDatasetDict({\n",
      "    train: IterableDataset({\n",
      "        features: ['image', 'filename', 'cocoid', 'caption'],\n",
      "        num_shards: 182\n",
      "    })\n",
      "    validation: IterableDataset({\n",
      "        features: ['image', 'filename', 'cocoid', 'caption'],\n",
      "        num_shards: 10\n",
      "    })\n",
      "    test: IterableDataset({\n",
      "        features: ['image', 'filename', 'cocoid', 'caption'],\n",
      "        num_shards: 9\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load COCO captions with streaming (no full download needed)\n",
    "ds = load_dataset(\"jxie/coco_captions\", streaming=True)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Tokenizer\n",
    "\n",
    "Load our BPE tokenizer with the `<image>` token we added in Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image> token ID: 32000\n",
      "<pad> token ID: 3\n",
      "</s> token ID: 2\n",
      "Vocab size: 32001\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer with <image> token\n",
    "tokenizer = Tokenizer.from_file(\"../bpe_tokenizer_with_image_tag.json\")\n",
    "\n",
    "# Important IDs we'll need\n",
    "IMAGE_TOKEN_ID = tokenizer.token_to_id(\"<image>\")  # 32000\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(\"<pad>\")      # Usually 0\n",
    "EOS_TOKEN_ID = tokenizer.token_to_id(\"</s>\")      # End of sequence\n",
    "\n",
    "print(f\"<image> token ID: {IMAGE_TOKEN_ID}\")\n",
    "print(f\"<pad> token ID: {PAD_TOKEN_ID}\")\n",
    "print(f\"</s> token ID: {EOS_TOKEN_ID}\")\n",
    "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Image Transform Pipeline\n",
    "\n",
    "ViT-B/16 was trained on ImageNet with specific preprocessing:\n",
    "\n",
    "1. **Resize(256)** - Resize shortest edge to 256 pixels (maintains aspect ratio)\n",
    "2. **CenterCrop(224)** - Crop center 224x224 (ViT input size)\n",
    "3. **ToTensor()** - Convert PIL Image to tensor, scales pixels from [0,255] to [0,1]\n",
    "4. **Normalize()** - Normalize with ImageNet statistics:\n",
    "   - Mean: [0.485, 0.456, 0.406] (RGB channel means from ImageNet)\n",
    "   - Std: [0.229, 0.224, 0.225] (RGB channel stds from ImageNet)\n",
    "\n",
    "**Why these specific values?**\n",
    "- The pretrained ViT learned features assuming this exact preprocessing\n",
    "- Using different normalization would produce garbage features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (640, 480)\n",
      "Transformed tensor shape: torch.Size([3, 224, 224])\n",
      "Tensor value range: [-2.12, 1.79]\n"
     ]
    }
   ],
   "source": [
    "# Image preprocessing pipeline - must match what ViT was trained with\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(256),                    # Resize shortest edge to 256\n",
    "    transforms.CenterCrop(224),                # Crop center 224x224\n",
    "    transforms.ToTensor(),                     # [0,255] -> [0,1], HWC -> CHW\n",
    "    transforms.Normalize(                      # ImageNet normalization\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Test on a sample image\n",
    "for sample in ds['train'].take(1):\n",
    "    img = sample['image']\n",
    "    print(f\"Original image size: {img.size}\")  # PIL Image size (W, H)\n",
    "    \n",
    "    img_tensor = image_transform(img)\n",
    "    print(f\"Transformed tensor shape: {img_tensor.shape}\")  # [3, 224, 224]\n",
    "    print(f\"Tensor value range: [{img_tensor.min():.2f}, {img_tensor.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Dataset Class\n",
    "\n",
    "Our dataset wraps HuggingFace's streaming dataset and:\n",
    "1. Applies image transforms\n",
    "2. Tokenizes captions with `<image>` prefix\n",
    "3. Handles padding and truncation\n",
    "\n",
    "**Caption format:**\n",
    "```\n",
    "Raw caption:    \"a brown dog running\"\n",
    "With prefix:    \"<image> a brown dog running\"\n",
    "Token IDs:      [32000, 214, 7532, 4485, 4274, <eos>]\n",
    "                  ^--- This position will be REPLACED by 196 vision tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Dataset for vision-language alignment training.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - image: preprocessed image tensor [3, 224, 224]\n",
    "    - input_ids: [<image>, caption_tokens..., <eos>]\n",
    "    - labels: same as input_ids (for next-token prediction loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,           # HuggingFace streaming dataset\n",
    "        tokenizer,            # Our BPE tokenizer\n",
    "        image_transform,      # torchvision transforms\n",
    "        max_seq_len=64,       # Max caption length (including <image> and <eos>)\n",
    "    ):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = image_transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Get special token IDs\n",
    "        self.image_token_id = tokenizer.token_to_id(\"<image>\")\n",
    "        self.pad_token_id = tokenizer.token_to_id(\"<pad>\") or 0\n",
    "        self.eos_token_id = tokenizer.token_to_id(\"</s>\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through the streaming dataset.\"\"\"\n",
    "        for sample in self.dataset:\n",
    "            try:\n",
    "                processed = self._process_sample(sample)\n",
    "                if processed is not None:\n",
    "                    yield processed\n",
    "            except Exception as e:\n",
    "                # Skip corrupted samples\n",
    "                continue\n",
    "    \n",
    "    def _process_sample(self, sample):\n",
    "        \"\"\"Process a single (image, caption) pair.\"\"\"\n",
    "        \n",
    "        # ----- Image Processing -----\n",
    "        image = sample['image']\n",
    "        \n",
    "        # Handle grayscale images (convert to RGB)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Apply transforms: resize, crop, normalize\n",
    "        image_tensor = self.transform(image)  # [3, 224, 224]\n",
    "        \n",
    "        # ----- Caption Processing -----\n",
    "        caption = sample['caption']\n",
    "        \n",
    "        # Prepend <image> token to caption\n",
    "        # This placeholder will be replaced by 196 vision tokens during forward pass\n",
    "        caption_with_image = f\"<image> {caption}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer.encode(caption_with_image)\n",
    "        token_ids = encoding.ids\n",
    "        \n",
    "        # Truncate (leave room for EOS)\n",
    "        if len(token_ids) > self.max_seq_len - 1:\n",
    "            token_ids = token_ids[:self.max_seq_len - 1]\n",
    "\n",
    "        # Add EOS\n",
    "        token_ids = token_ids + [self.eos_token_id]\n",
    "\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        \n",
    "        # Pad if too short\n",
    "        padding_length = self.max_seq_len - len(token_ids)\n",
    "        if padding_length > 0:\n",
    "            token_ids = token_ids + [self.pad_token_id] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        \n",
    "        # Labels are same as input_ids for language modeling\n",
    "        # During loss computation, we'll shift and mask appropriately\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        return {\n",
    "            'image': image_tensor,           # [3, 224, 224]\n",
    "            'input_ids': input_ids,          # [max_seq_len]\n",
    "            'attention_mask': attention_mask, # [max_seq_len]\n",
    "            'labels': labels                  # [max_seq_len]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create DataLoader\n",
    "\n",
    "The DataLoader handles batching. Since we already padded to `max_seq_len`, \n",
    "we can use the default collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoader created!\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "train_dataset = VisionLanguageDataset(\n",
    "    hf_dataset=ds['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    image_transform=image_transform,\n",
    "    max_seq_len=128  # Plenty for COCO captions\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=0  # Set to 0 for streaming datasets\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Pipeline\n",
    "\n",
    "Let's verify everything works correctly by inspecting a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch contents:\n",
      "  image shape:          torch.Size([4, 3, 224, 224])\n",
      "  input_ids shape:      torch.Size([4, 128])\n",
      "  attention_mask shape: torch.Size([4, 128])\n",
      "  labels shape:         torch.Size([4, 128])\n",
      "\n",
      "First sample in batch:\n",
      "  input_ids: [32000, 297, 5403, 7289, 214, 1827, 310, 663, 2519, 3958, 214, 9556, 17, 177, 2, 3, 3, 3, 3, 3]...\n",
      "  First token is <image>? True\n"
     ]
    }
   ],
   "source": [
    "# Get one batch to verify\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "print(f\"  image shape:          {batch['image'].shape}\")          # [B, 3, 224, 224]\n",
    "print(f\"  input_ids shape:      {batch['input_ids'].shape}\")      # [B, max_seq_len]\n",
    "print(f\"  attention_mask shape: {batch['attention_mask'].shape}\") # [B, max_seq_len]\n",
    "print(f\"  labels shape:         {batch['labels'].shape}\")         # [B, max_seq_len]\n",
    "\n",
    "print(\"\\nFirst sample in batch:\")\n",
    "print(f\"  input_ids: {batch['input_ids'][0][:20].tolist()}...\")   # First 20 tokens\n",
    "print(f\"  First token is <image>? {batch['input_ids'][0][0].item() == IMAGE_TOKEN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded caption:  A woman wearing a net on her head cutting a cake. \n"
     ]
    }
   ],
   "source": [
    "# Decode a sample to verify tokenization\n",
    "sample_ids = batch['input_ids'][0].tolist()\n",
    "\n",
    "# Remove padding for cleaner output\n",
    "sample_ids_no_pad = [t for t in sample_ids if t != PAD_TOKEN_ID]\n",
    "\n",
    "decoded = tokenizer.decode(sample_ids_no_pad)\n",
    "print(f\"Decoded caption: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we built:**\n",
    "- Image transform pipeline matching ViT preprocessing\n",
    "- Dataset class that yields `(image, input_ids, attention_mask, labels)`\n",
    "- Each caption starts with `<image>` token (ID: 32000)\n",
    "\n",
    "**Next steps (Phase 3):**\n",
    "- Modify LLM embedding layer to handle the new vocab size (32001)\n",
    "- Create function to replace `<image>` embedding with 196 vision tokens\n",
    "- Build the full VLM forward pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
