{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddef422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tomllib\n",
    "import sys\n",
    "\n",
    "# Add vision_language_alignment to path first (for dataset)\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from dataset import get_dataloaders\n",
    "\n",
    "# Add text_pretraining to path (for Llama model)\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent / \"text_pretraining\"))\n",
    "from model import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77913095",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "CONFIG_PATH = Path.cwd().parent.parent / \"config.toml\"\n",
    "\n",
    "with open(CONFIG_PATH, \"rb\") as f:\n",
    "    cfg = tomllib.load(f)\n",
    "    cfg[\"d_head\"] = cfg[\"d_model\"] // cfg[\"n_heads\"]\n",
    "    cfg[\"kv_d_head\"] = cfg[\"d_model\"] // cfg[\"n_kv_heads\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8600d487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 7,\n",
       " 'batches_per_epoch': 20000,\n",
       " 'vocab_size': 32000,\n",
       " 'd_model': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_kv_heads': 4,\n",
       " 'n_layers': 12,\n",
       " 'max_seq_len': 1024,\n",
       " 'num_experts': 8,\n",
       " 'num_experts_per_tok': 2,\n",
       " 'moe_layer_freq': 2,\n",
       " 'd_expert': 1024,\n",
       " 'd_ff_standard': 2048,\n",
       " 'rope_layers_ratio': 0.75,\n",
       " 'rope_theta': 10000,\n",
       " 'chunk_size': 512,\n",
       " 'vision_hidden_size': 768,\n",
       " 'patch_size': 16,\n",
       " 'image_size': 224,\n",
       " 'd_head': 64,\n",
       " 'kv_d_head': 192}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc99273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision encoder for LLaMA 4 vision-language alignment.\n",
    "    Uses pretrained ViT-B/16 (frozen) with a trainable MLP projector.\n",
    "\n",
    "    Flow: Image -> Pretrained ViT -> MLP Projector -> LLM-compatible embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm_embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained ViT-B/16 (trained on ImageNet)\n",
    "        vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Extract components\n",
    "        self.patch_embed = vit.conv_proj  # [B,3,224,224] -> [B,768,14,14]\n",
    "        self.pos_embed = vit.encoder.pos_embedding  # [1, 197, 768] (includes CLS)\n",
    "        self.encoder = vit.encoder.layers  # 12 transformer blocks\n",
    "        self.norm = vit.encoder.ln  # Final LayerNorm\n",
    "\n",
    "        # Freeze entire vision encoder\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Trainable 2-layer MLP projector (like LLaVA)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 4), nn.GELU(), nn.Linear(768 * 4, llm_embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, 224, 224]\n",
    "\n",
    "        # Patch embedding via Conv2d\n",
    "        x = self.patch_embed(x)  # [B, 768, 14, 14]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, 196, 768]\n",
    "\n",
    "        # Add positional embeddings (skip CLS token position at index 0)\n",
    "        x = x + self.pos_embed[:, 1:, :]  # [B, 196, 768]\n",
    "\n",
    "        # Transformer encoder\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)  # [B, 196, 768]\n",
    "\n",
    "        # Project to LLM embedding space\n",
    "        x = self.proj(x)  # [B, 196, llm_embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "# # Test the vision encoder\n",
    "# vision_encoder = VisionEncoder(llm_embed_dim=768)\n",
    "\n",
    "# # Check trainable vs frozen params\n",
    "# trainable = sum(p.numel() for p in vision_encoder.parameters() if p.requires_grad)\n",
    "# total = sum(p.numel() for p in vision_encoder.parameters())\n",
    "# print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "\n",
    "# # Forward pass\n",
    "# x = torch.randn(2, 3, 224, 224)\n",
    "# out = vision_encoder(x)\n",
    "# print(f\"Input:  {x.shape}\")\n",
    "# print(f\"Output: {out.shape}\")  # [B, 196, 768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b9f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size,text_pretrained_model,llm_embed_dim,ckpt_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = VisionEncoder(llm_embed_dim=llm_embed_dim)\n",
    "        self.text_pretrained_model = text_pretrained_model\n",
    "        self._load_model_from_ckpt(ckpt_path)\n",
    "\n",
    "        # Embedding layer\n",
    "        new_emb = nn.Embedding(vocab_size + 1, llm_embed_dim)\n",
    "        new_emb.weight.data[:vocab_size, :] = self.text_pretrained_model.emb.emb.weight.data\n",
    "        new_emb.weight.data[vocab_size, :] = torch.randn(llm_embed_dim) * (llm_embed_dim ** -0.5)\n",
    "        self.text_pretrained_model.emb.emb = new_emb\n",
    "\n",
    "        # Output projection\n",
    "        old_proj = self.text_pretrained_model.proj_vocab\n",
    "        new_proj = nn.Linear(llm_embed_dim, vocab_size + 1, bias=False)\n",
    "        new_proj.weight.data[:vocab_size, :] = old_proj.weight.data\n",
    "        new_proj.weight.data[vocab_size, :] = torch.randn(llm_embed_dim) * 0.02\n",
    "        self.text_pretrained_model.proj_vocab = new_proj\n",
    "\n",
    "        for param in self.text_pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.image_token_id = 32000\n",
    "        self.pad_token_id = 3\n",
    "        \n",
    "    def _load_model_from_ckpt(self,ckpt_path):\n",
    "        if os.path.exists(ckpt_path):\n",
    "            print(f\"Loading checkpoint from: {ckpt_path}\")\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            self.text_pretrained_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            print(\"Checkpoint loaded successfully\")\n",
    "        else:\n",
    "            print(f\"File not found at the path provided\")\n",
    "\n",
    "    def forward(self,image,input_ids):\n",
    "        vision_embeds = self.vision_encoder(image)  # [B, 196, llm_embed_dim]\n",
    "        text_embeds = self.text_pretrained_model.emb(input_ids)  # [B, seq_len, llm_embed_dim]\n",
    "        combined = torch.cat([vision_embeds,text_embeds[:,1:,:]],dim=1) # [B, 196 + seq_len-1, llm_embed_dim]\n",
    "        \n",
    "        print(vision_embeds.shape, text_embeds.shape)\n",
    "        print(combined.shape)  # [B, 196 + seq_len-1, llm_embed_dim]\n",
    "\n",
    "        for i, decoder in enumerate(self.text_pretrained_model.decoder_layers):\n",
    "            combined = decoder(i, combined)\n",
    "            print(i,combined.shape)\n",
    "\n",
    "        combined = self.text_pretrained_model.rms_norm(combined) # [B, 196 + seq_len-1, llm_embed_dim]\n",
    "        logits = self.text_pretrained_model.proj_vocab(combined) # [B, 196 + seq_len-1, vocab_size+1]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51f225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pretrained_model = Llama(\n",
    "        vocab_size=cfg[\"vocab_size\"],\n",
    "        n_layers=cfg[\"n_layers\"],\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        d_head=cfg[\"d_head\"],\n",
    "        n_heads=cfg[\"n_heads\"],\n",
    "        n_kv_heads=cfg[\"n_kv_heads\"],\n",
    "        kv_d_head=cfg[\"kv_d_head\"],\n",
    "        d_ff_standard=cfg[\"d_ff_standard\"],\n",
    "        num_experts=cfg[\"num_experts\"],\n",
    "        num_experts_per_tok=cfg[\"num_experts_per_tok\"],\n",
    "        d_expert=cfg[\"d_expert\"],\n",
    "        rope_layers_ratio=cfg[\"rope_layers_ratio\"],\n",
    "        chunk_size=cfg[\"chunk_size\"],\n",
    "        rope_theta=cfg[\"rope_theta\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6a1243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /home/smedar/code_files/llama4-from-scratch/text_pretraining/checkpoints/best.pt\n",
      "Checkpoint loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model = VisionLanguageModel(vocab_size=cfg[\"vocab_size\"],\n",
    "                            text_pretrained_model=text_pretrained_model,\n",
    "                            llm_embed_dim=cfg['d_model'],\n",
    "                            ckpt_path=\"/home/smedar/code_files/llama4-from-scratch/text_pretraining/checkpoints/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7510ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 4,722,432 / 470,069,772 (1.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check trainable vs frozen params\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573364fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loaded tokenizer from /home/smedar/code_files/llama4-from-scratch/vision_language_alignment/bpe_tokenizer_with_image_tag.json\n",
      "Vocab size: 32001\n",
      "Setting up image transforms...\n",
      "Loading COCO captions dataset (streaming)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e7018a936d4e9bb2fe09eb863b5d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Max sequence length: 128\n",
      "\n",
      "Testing train loader...\n",
      "  image shape:          torch.Size([1, 3, 224, 224])\n",
      "  input_ids shape:      torch.Size([1, 128])\n",
      "  attention_mask shape: torch.Size([1, 128])\n",
      "  labels shape:         torch.Size([1, 128])\n",
      "  First token (should be 32000): 32000\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataloaders(batch_size=1, max_seq_len=128)\n",
    "\n",
    "print(\"\\nTesting train loader...\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"  image shape:          {batch['image'].shape}\")\n",
    "print(f\"  input_ids shape:      {batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"  labels shape:         {batch['labels'].shape}\")\n",
    "print(f\"  First token (should be 32000): {batch['input_ids'][0][0].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbc4639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768]) torch.Size([1, 128, 768])\n",
      "torch.Size([1, 323, 768])\n",
      "0 torch.Size([1, 323, 768])\n",
      "1 torch.Size([1, 323, 768])\n",
      "2 torch.Size([1, 323, 768])\n",
      "3 torch.Size([1, 323, 768])\n",
      "4 torch.Size([1, 323, 768])\n",
      "5 torch.Size([1, 323, 768])\n",
      "6 torch.Size([1, 323, 768])\n",
      "7 torch.Size([1, 323, 768])\n",
      "8 torch.Size([1, 323, 768])\n",
      "9 torch.Size([1, 323, 768])\n",
      "10 torch.Size([1, 323, 768])\n",
      "11 torch.Size([1, 323, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 323, 32001])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch['image'],batch['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59fa0b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768]) torch.Size([1, 128, 768])\n",
      "torch.Size([1, 323, 768])\n",
      "0 torch.Size([1, 323, 768])\n",
      "1 torch.Size([1, 323, 768])\n",
      "2 torch.Size([1, 323, 768])\n",
      "3 torch.Size([1, 323, 768])\n",
      "4 torch.Size([1, 323, 768])\n",
      "5 torch.Size([1, 323, 768])\n",
      "6 torch.Size([1, 323, 768])\n",
      "7 torch.Size([1, 323, 768])\n",
      "8 torch.Size([1, 323, 768])\n",
      "9 torch.Size([1, 323, 768])\n",
      "10 torch.Size([1, 323, 768])\n",
      "11 torch.Size([1, 323, 768])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for i,b in enumerate(train_loader):\n",
    "    image = b['image'].to(device)\n",
    "    input_ids = b['input_ids'].to(device)\n",
    "    attention_mask = b['attention_mask'].to(device)\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    logits = model(image,input_ids)  # [B, 323, vocab_size+1]\n",
    " \n",
    "    labels = torch.full((logits.size(0),323),-100, device=device) # [B,323]\n",
    "    labels[:,196:] = input_ids[:,1:] # caption_tokens (without <image>)\n",
    "    labels[labels == model.pad_token_id] = -100\n",
    "\n",
    "    # Reshape for cross-entropy\n",
    "    logits_flat = logits[:, :-1, :].reshape(-1, cfg['vocab_size']+1)  # [B*322, vocab_size+1]\n",
    "    labels_flat = labels[:, 1:].reshape(-1)                   # [B*322]\n",
    "\n",
    "    loss = loss_fn(logits_flat, labels_flat)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    opt.step()\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
