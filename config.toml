# LLaMA 4 Transformer Configuration

# Dataset settings (RTX 3090 24GB)
batch_size = 7
batches_per_epoch = 20000
vocab_size = 32000

# Core dimensions
d_model = 768
n_heads = 12
n_kv_heads = 4  # GQA (grouped query attention)
n_layers = 12
max_seq_len = 1024

# MoE config (simplified)
num_experts = 8
num_experts_per_tok = 2  # Top-2 routing
moe_layer_freq = 2  # Every 2nd layer is MoE
d_expert = 1024  # Increased for better capacity

# Dense config
d_ff_standard = 2048

# iRoPE config
rope_layers_ratio = 0.75  # 3 RoPE : 1 NoPE pattern
rope_theta = 10000
chunk_size = 512  # Chunked attention window

# Vision Encoder (ViT-B/16)
vision_hidden_size = 768
patch_size = 16
image_size = 224  # ViT-B/16 standard

# ==================== VLM Alignment Training (Phase 2) ====================
# Only trains MLP projector (~4.7M params), ViT and LLM frozen

# Training hyperparameters (RTX 3090 24GB)
vlm_batch_size = 48         # Uses ~22GB VRAM
vlm_epochs = 3              # 3 passes through full dataset
vlm_batches_per_epoch = -1  # -1 = full epoch (567k / 48 = ~12k batches)
vlm_learning_rate = 2e-4    # Slightly higher for projector-only training
vlm_weight_decay = 0.01
vlm_max_grad_norm = 1.0

# Data
vlm_max_seq_len = 64        # Captions are short (95th pct ~18 tokens, 64 is plenty)

# Scheduler
vlm_warmup_steps = 360      # ~1% of 36k total steps

# Validation
vlm_val_batches = 500       # 500 * 48 = 24k samples for validation

# ==================== Visual Instruction Tuning (Phase 3) ====================
# Trains MLP projector (~4.7M) + LLM (~380M), ViT frozen (~86M)
# Uses LLaVA-Instruct-150K dataset (~150K conversations with COCO images)

# Training hyperparameters (RTX 3090 24GB)
instruct_batch_size = 12          # ~23GB VRAM usage
instruct_epochs = 1               # 1 pass usually enough for instruction tuning
instruct_batches_per_epoch = -1     # -1 = full epoch (dataset downloaded locally)
instruct_learning_rate = 2e-5     # Lower LR (LLM already pretrained)
instruct_weight_decay = 0.01
instruct_max_grad_norm = 1.0

# Data
instruct_max_seq_len = 512        # Conversations longer than captions (max ~440 tokens)

# Scheduler
instruct_warmup_steps = 1000      # ~3% of 35k steps, good for stability

# Validation
instruct_val_batches = 500        # 500 * 4 = 2k samples for validation

# Logging
instruct_log_interval = 200       # Log every 200 batches

# ==================== DPO Preference Tuning (Phase 4) ====================
# Direct Preference Optimization using RLAIF-V dataset (83K preference pairs)
# Trains full model (same as Phase 3) to prefer chosen over rejected responses

# Training hyperparameters (RTX 3090 24GB)
dpo_batch_size = 4                # Lower due to 2x forward passes (chosen + rejected)
dpo_epochs = 1                    # 1 epoch usually sufficient
dpo_batches_per_epoch = -1        # -1 = full epoch
dpo_learning_rate = 5e-6          # Very low LR for preference tuning
dpo_weight_decay = 0.01
dpo_beta = 0.1                    # DPO temperature parameter

# Data
dpo_max_seq_len = 512

# Scheduler
dpo_warmup_steps = 100

# Validation
dpo_val_batches = 200

# Logging
dpo_log_interval = 50
dpo_save_interval = 500