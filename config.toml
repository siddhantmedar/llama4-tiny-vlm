# LLaMA 4 Transformer Configuration

# Dataset settings (RTX 3090 24GB)
batch_size = 7
batches_per_epoch = 20000
vocab_size = 32000

# Core dimensions
d_model = 768
n_heads = 12
n_kv_heads = 4  # GQA (grouped query attention)
n_layers = 12
max_seq_len = 1024

# MoE config (simplified)
num_experts = 8
num_experts_per_tok = 2  # Top-2 routing
moe_layer_freq = 2  # Every 2nd layer is MoE
d_expert = 1024  # Increased for better capacity

# Dense config
d_ff_standard = 2048

# iRoPE config
rope_layers_ratio = 0.75  # 3 RoPE : 1 NoPE pattern
rope_theta = 10000
chunk_size = 512  # Chunked attention window

# Vision Encoder (ViT-B/16)
vision_hidden_size = 768
patch_size = 16
image_size = 224  # ViT-B/16 standard

# ==================== VLM Alignment Training (Phase 2) ====================
# Only trains MLP projector (~4.7M params), ViT and LLM frozen

# Training hyperparameters (RTX 3090 24GB)
vlm_batch_size = 48         # Uses ~22GB VRAM
vlm_epochs = 3              # 3 passes through full dataset
vlm_batches_per_epoch = -1  # -1 = full epoch (567k / 48 = ~12k batches)
vlm_learning_rate = 2e-4    # Slightly higher for projector-only training
vlm_weight_decay = 0.01
vlm_max_grad_norm = 1.0

# Data
vlm_max_seq_len = 64        # Captions are short (95th pct ~18 tokens, 64 is plenty)

# Scheduler
vlm_warmup_steps = 360      # ~1% of 36k total steps

# Validation
vlm_val_batches = 500       # 500 * 48 = 24k samples for validation