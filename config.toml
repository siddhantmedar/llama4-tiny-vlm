# LLaMA 4 Transformer Configuration

# Dataset settings
batch_size = 64
batches_per_epoch = 100
vocab_size = 32000

# Core dimensions
d_model = 768
n_heads = 12
n_kv_heads = 4  # GQA (grouped query attention)
n_layers = 12
max_seq_len = 2048  # Keep short for speed

# MoE config (simplified)
num_experts = 8  # Not 128, just 8 for learning
num_experts_per_tok = 2  # Top-2 routing
moe_layer_freq = 2  # Every 2nd layer is MoE

# iRoPE config
rope_layers_ratio = 0.75  # 3 RoPE : 1 NoPE pattern
rope_theta = 10000
chunk_size = 512  # Chunked attention window

# Vision (Stage 2+)
vision_hidden_size = 768
patch_size = 16
image_size = 224  # Smaller images = faster