{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da191e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tomllib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ccf36",
   "metadata": {},
   "outputs": [],
   "source": "def set_seed(seed=313):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nset_seed()\n\nDEVICE = \"cuda\"\nCONFIG_PATH = Path.cwd().parent.parent / \"config.toml\"\n\nwith open(CONFIG_PATH, \"rb\") as f:\n    cfg = tomllib.load(f)\n    assert (\n        divmod(cfg[\"d_model\"], cfg[\"n_heads\"])[1] == 0\n    ), \"d_model should be divisble by n_heads\"\n    assert (\n        divmod(cfg[\"n_heads\"], cfg[\"n_kv_heads\"])[1] == 0\n    ), \"n_heads should be divisble by n_kv_heads\"\n\n    cfg[\"d_head\"] = cfg[\"d_model\"] // cfg[\"n_heads\"]\n    cfg[\"kv_d_head\"] = cfg[\"d_model\"] // cfg[\"n_kv_heads\"]"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c1a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        nn.init.normal_(self.emb.weight, mean=0, std=(d_model) ** -0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        den = ((x**2).sum(dim=-1, keepdim=True) / x.size(-1) + 1e-6) ** 0.5\n",
    "        return (x / den) * self.gamma\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        n_heads,\n",
    "        n_kv_heads,\n",
    "        kv_d_head,\n",
    "        rope_layers_ratio,\n",
    "        chunk_size,\n",
    "        rope_theta,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rope_period = int(1 / (1 - rope_layers_ratio))\n",
    "        self.rope_theta = rope_theta\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.kv_d_head = kv_d_head\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "\n",
    "        self.q_norm = RMSNorm(d_head)\n",
    "        self.k_norm = RMSNorm(d_head)\n",
    "\n",
    "        self.w_q = nn.Linear(self.d_model, self.n_heads * self.d_head, bias=False)\n",
    "        self.w_k = nn.Linear(self.d_model, self.n_kv_heads * self.d_head, bias=False)\n",
    "        self.w_v = nn.Linear(self.d_model, self.n_kv_heads * self.d_head, bias=False)\n",
    "\n",
    "        self.temp_scale = nn.Parameter(torch.tensor(0.1))\n",
    "        self.proj_out = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _apply_rope(x, base=10_000):\n",
    "        \"\"\"\n",
    "        For each pair of dimensions (2i, 2i+1):\n",
    "        q_rotated[2i]   = q[2i] * cos(m*θᵢ) - q[2i+1] * sin(m*θᵢ)\n",
    "        q_rotated[2i+1] = q[2i] * sin(m*θᵢ) + q[2i+1] * cos(m*θᵢ)\n",
    "\n",
    "        Where theta_i = 10000^{-2i/d}\n",
    "        \"\"\"\n",
    "        # x: [b,s,n_heads,d_head]\n",
    "        s, _, dim = x.shape[1:]\n",
    "        x_rotated = torch.zeros_like(x)\n",
    "\n",
    "        m = torch.arange(s, device=x.device).unsqueeze(-1)  # pos\n",
    "        i = torch.arange(0, dim, 2, device=x.device)  # dim idx\n",
    "\n",
    "        theta = base ** (-i / dim)\n",
    "\n",
    "        x1, x2 = x[:, :, :, 0::2], x[:, :, :, 1::2]\n",
    "\n",
    "        angle = (m * theta).unsqueeze(-2)\n",
    "        x_rotated[:, :, :, 0::2] = x1 * torch.cos(angle) - x2 * torch.sin(angle)\n",
    "        x_rotated[:, :, :, 1::2] = x1 * torch.sin(angle) + x2 * torch.cos(angle)\n",
    "\n",
    "        return x_rotated\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_chunk_mask(seq_length, chunk_size, device):\n",
    "        \"\"\"RoPE layers: attend within chunks only\"\"\"\n",
    "        mask = torch.zeros(seq_length, seq_length, device=device)\n",
    "\n",
    "        for i in range(0, seq_length, chunk_size):\n",
    "            end = min(seq_length, i + chunk_size)\n",
    "            mask[i:end, i:end] = 1\n",
    "\n",
    "        mask = mask * torch.tril(torch.ones_like(mask))\n",
    "\n",
    "        return ~mask.bool()\n",
    "\n",
    "    def forward(self, idx, x):\n",
    "        b, s, _ = x.shape\n",
    "\n",
    "        q = self.w_q(x).view(b, s, self.n_heads, self.d_head)  # [b,s,n_heads,d_head]\n",
    "        k = self.w_k(x).view(\n",
    "            b, s, self.n_kv_heads, self.d_head\n",
    "        )  # [b,s,n_kv_heads,d_head]\n",
    "        v = self.w_v(x).view(\n",
    "            b, s, self.n_kv_heads, self.d_head\n",
    "        )  # [b,s,n_kv_heads,d_head]\n",
    "\n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "\n",
    "        is_rope_layer = (idx + 1) % self.rope_period != 0\n",
    "\n",
    "        if is_rope_layer:\n",
    "            q = MultiHeadAttention._apply_rope(q, self.rope_theta)  # rotated\n",
    "            k = MultiHeadAttention._apply_rope(k, self.rope_theta)  # rotated\n",
    "\n",
    "        # repeat KV heads to match Q heads\n",
    "        n_rep = self.n_heads // self.n_kv_heads\n",
    "        k = torch.repeat_interleave(k, repeats=n_rep, dim=2)  # [b,s,n_heads,d_head]\n",
    "        v = torch.repeat_interleave(v, repeats=n_rep, dim=2)  # [b,s,n_heads,d_head]\n",
    "\n",
    "        # Transpose for attention\n",
    "        q = q.transpose(1, 2)  # [b,n_heads,s,d_head]\n",
    "        k = k.transpose(1, 2)  # [b,n_heads,s,d_head]\n",
    "        v = v.transpose(1, 2)  # [b,n_heads,s,d_head]\n",
    "\n",
    "        scores = q @ k.transpose(-1, -2) / (self.d_head**0.5)  # [b,n_heads,s,s]\n",
    "\n",
    "        # causal mask\n",
    "        if is_rope_layer:\n",
    "            mask = MultiHeadAttention._create_chunk_mask(s, self.chunk_size, x.device)\n",
    "        else:\n",
    "            # NoPE: full causal attention\n",
    "            mask = torch.triu(torch.ones(s, s, device=x.device), diagonal=1).bool()\n",
    "\n",
    "        scores = torch.masked_fill(scores, mask, float(\"-inf\"))\n",
    "\n",
    "        if not is_rope_layer:\n",
    "            # temperature scaling prevents attention flattening on long sequences\n",
    "            scores = scores * self.temp_scale\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        output = attn_weights @ v  # [b,n_heads,s,d_head]\n",
    "        output = output.contiguous().transpose(1, 2).reshape(b, s, self.d_model)\n",
    "\n",
    "        return self.proj_out(output)  # [b,s,d_model]\n",
    "\n",
    "\n",
    "class DenseFFNBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff_standard):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_gate = nn.Linear(d_model, d_ff_standard, bias=False)\n",
    "        self.w_up = nn.Linear(d_model, d_ff_standard, bias=False)\n",
    "        self.w_down = nn.Linear(d_ff_standard, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [b,s,d_model]\n",
    "        out1 = F.silu(self.w_gate(x))  # [b,s,dff]\n",
    "        out2 = self.w_up(x)  # [b,s,dff]\n",
    "        return self.w_down((out1 * out2))  # [b,s,d_model]\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, d_model, d_expert):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_gate = nn.Linear(d_model, d_expert, bias=False)\n",
    "        self.w_up = nn.Linear(d_model, d_expert, bias=False)\n",
    "        self.w_down = nn.Linear(d_expert, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [b*s,d_model]\n",
    "        out1 = F.silu(self.w_gate(x))  # [b*s,dexp]\n",
    "        out2 = self.w_up(x)  # [b*s,dexp]\n",
    "        return self.w_down((out1 * out2))  # [b*s,d_model]\n",
    "\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, num_experts, num_experts_per_tok, d_model):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.w_router = nn.Linear(d_model, num_experts, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.w_router(x)  # [b,s,num_experts]\n",
    "        router_probs = F.softmax(logits, dim=-1)  # [b,s,num_experts]\n",
    "        top_k_weights, top_k_indices = torch.topk(\n",
    "            router_probs, self.num_experts_per_tok, dim=-1\n",
    "        )\n",
    "        top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n",
    "        return top_k_weights, top_k_indices, router_probs\n",
    "\n",
    "\n",
    "class MoEFFNBlock(nn.Module):\n",
    "    def __init__(self, num_experts, num_experts_per_tok, d_expert, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_experts = num_experts\n",
    "        self.gate = GatingNetwork(num_experts, num_experts_per_tok, self.d_model)\n",
    "\n",
    "        self.shared_expert = Expert(d_model, d_expert)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [Expert(d_model, d_expert) for _ in range(self.num_experts)]\n",
    "        )\n",
    "\n",
    "        self.aux_loss = 0.0\n",
    "\n",
    "    def _load_balancing_loss(self, router_probs, indices):\n",
    "        \"\"\"\n",
    "        encourages uniform expert usage. without this, router collapses to always picking same experts.\n",
    "        note: the minimum loss is always 1.0 regardless of number of experts!\n",
    "        \"\"\"\n",
    "        # router_probs: [b, s, num_experts] - pre-topk softmax probs\n",
    "        # indices: [b, s, topk] - selected expert indices\n",
    "\n",
    "        router_probs_flat = router_probs.view(\n",
    "            -1, self.num_experts\n",
    "        )  # [b*s, num_experts]\n",
    "        indices_flat = indices.view(-1)  # [b*s*topk]\n",
    "\n",
    "        # f_i = fraction of tokens routed to expert i\n",
    "        counts = torch.bincount(indices_flat, minlength=self.num_experts).float()\n",
    "        f = counts / indices_flat.numel()\n",
    "\n",
    "        # P_i = mean routing probability for expert i\n",
    "        P = router_probs_flat.mean(dim=0)\n",
    "\n",
    "        # Loss = N * Σ(f_i * P_i)\n",
    "        # Minimized when both f and P are uniform (1/N each)\n",
    "        return self.num_experts * (f * P).sum()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, d_model = x.shape\n",
    "        weights, indices, router_probs = self.gate(x)\n",
    "\n",
    "        x_flatten = x.contiguous().view(-1, self.d_model)  # [b*s,d_model]\n",
    "\n",
    "        output = torch.zeros_like(x_flatten)  # [b*s,d]\n",
    "\n",
    "        shared_output = self.shared_expert(x_flatten)  # [b*s,d_model]\n",
    "\n",
    "        # for each expert, gather tokens assigned to it\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            expert_mask = indices == expert_idx  # [b,s,topk]\n",
    "            token_mask = expert_mask.any(dim=-1).flatten(start_dim=0)  # [b*s]\n",
    "            if token_mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            token_weights = weights[expert_mask]  # num_assigned\n",
    "\n",
    "            expert_input = x_flatten[token_mask]  # [num_assigned,d_model]\n",
    "            expert_output = self.experts[expert_idx](\n",
    "                expert_input\n",
    "            )  # [num_assigned,d_model]\n",
    "            output[token_mask] += (\n",
    "                token_weights.unsqueeze(-1) * expert_output\n",
    "            )  # [b*s,d_model]\n",
    "\n",
    "        output = output + shared_output\n",
    "\n",
    "        self.aux_loss = self._load_balancing_loss(router_probs, indices)\n",
    "\n",
    "        return output.contiguous().view(b, s, d_model)  # [b,s,d_model]\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model, d_ff_standard, num_experts, num_experts_per_tok, d_expert\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense_ffn_block = DenseFFNBlock(d_model, d_ff_standard)\n",
    "        self.moe_ffn_block = MoEFFNBlock(\n",
    "            num_experts, num_experts_per_tok, d_expert, d_model\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, x):\n",
    "        return self.dense_ffn_block(x) if idx % 2 == 0 else self.moe_ffn_block(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        n_heads,\n",
    "        n_kv_heads,\n",
    "        kv_d_head,\n",
    "        d_ff_standard,\n",
    "        num_experts,\n",
    "        num_experts_per_tok,\n",
    "        d_expert,\n",
    "        rope_layers_ratio,\n",
    "        chunk_size,\n",
    "        rope_theta,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(\n",
    "            d_model,\n",
    "            d_head,\n",
    "            n_heads,\n",
    "            n_kv_heads,\n",
    "            kv_d_head,\n",
    "            rope_layers_ratio,\n",
    "            chunk_size,\n",
    "            rope_theta,\n",
    "        )\n",
    "        self.ffn = FFN(\n",
    "            d_model, d_ff_standard, num_experts, num_experts_per_tok, d_expert\n",
    "        )\n",
    "        self.rms_norm1 = RMSNorm(d_model)\n",
    "        self.rms_norm2 = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, idx, x):\n",
    "        x_original = x\n",
    "        x = self.rms_norm1(x)\n",
    "        x = self.mha(idx, x) + x_original\n",
    "\n",
    "        x_original = x\n",
    "        x = self.rms_norm2(x)\n",
    "        x = self.ffn(idx, x)\n",
    "\n",
    "        return x + x_original\n",
    "\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        n_heads,\n",
    "        n_kv_heads,\n",
    "        kv_d_head,\n",
    "        d_ff_standard,\n",
    "        num_experts,\n",
    "        num_experts_per_tok,\n",
    "        d_expert,\n",
    "        rope_layers_ratio,\n",
    "        chunk_size,\n",
    "        rope_theta,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = Embedding(vocab_size, d_model)\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                Decoder(\n",
    "                    d_model,\n",
    "                    d_head,\n",
    "                    n_heads,\n",
    "                    n_kv_heads,\n",
    "                    kv_d_head,\n",
    "                    d_ff_standard,\n",
    "                    num_experts,\n",
    "                    num_experts_per_tok,\n",
    "                    d_expert,\n",
    "                    rope_layers_ratio,\n",
    "                    chunk_size,\n",
    "                    rope_theta,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.rms_norm = RMSNorm(d_model)\n",
    "        self.proj_vocab = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.aux_loss = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.emb(x)\n",
    "\n",
    "        aux_loss = 0.0\n",
    "\n",
    "        for i, decoder in enumerate(self.decoder_layers):\n",
    "            out = decoder(i, out)\n",
    "\n",
    "            if hasattr(decoder.ffn, \"moe_ffn_block\"):\n",
    "                aux_loss += decoder.ffn.moe_ffn_block.aux_loss\n",
    "\n",
    "        out = self.rms_norm(out)\n",
    "        logits = self.proj_vocab(out)\n",
    "\n",
    "        self.aux_loss = aux_loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fd9056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 900, (2, 5))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d1cab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Llama(\n",
       "  (emb): Embedding(\n",
       "    (emb): Embedding(32000, 768)\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-11): 12 x Decoder(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (proj_out): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (dense_ffn_block): DenseFFNBlock(\n",
       "          (w_gate): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (w_up): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (w_down): Linear(in_features=2048, out_features=768, bias=False)\n",
       "        )\n",
       "        (moe_ffn_block): MoEFFNBlock(\n",
       "          (gate): GatingNetwork(\n",
       "            (w_router): Linear(in_features=768, out_features=8, bias=False)\n",
       "          )\n",
       "          (shared_expert): Expert(\n",
       "            (w_gate): Linear(in_features=768, out_features=308, bias=False)\n",
       "            (w_up): Linear(in_features=768, out_features=308, bias=False)\n",
       "            (w_down): Linear(in_features=308, out_features=768, bias=False)\n",
       "          )\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x Expert(\n",
       "              (w_gate): Linear(in_features=768, out_features=308, bias=False)\n",
       "              (w_up): Linear(in_features=768, out_features=308, bias=False)\n",
       "              (w_down): Linear(in_features=308, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rms_norm1): RMSNorm()\n",
       "      (rms_norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (rms_norm): RMSNorm()\n",
       "  (proj_vocab): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Llama(\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    d_head=cfg[\"d_head\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    n_kv_heads=cfg[\"n_kv_heads\"],\n",
    "    kv_d_head=cfg[\"kv_d_head\"],\n",
    "    d_ff_standard=cfg[\"d_ff_standard\"],\n",
    "    num_experts=cfg[\"num_experts\"],\n",
    "    num_experts_per_tok=cfg[\"num_experts_per_tok\"],\n",
    "    d_expert=cfg[\"d_expert\"],\n",
    "    rope_layers_ratio=cfg[\"rope_layers_ratio\"],\n",
    "    chunk_size=cfg[\"chunk_size\"],\n",
    "    rope_theta=cfg[\"rope_theta\"],\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdd5972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 32000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}