{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Text Generation Inference\n",
    "\n",
    "Generate text samples from the pretrained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tomllib\n",
    "from tokenizers import Tokenizer\n",
    "from text_pretraining.model import Llama\n",
    "\n",
    "# Load config\n",
    "with open(PROJECT_ROOT / \"config.toml\", \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(str(PROJECT_ROOT / \"text_pretraining\" / \"bpe_tokenizer.json\"))\n",
    "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded checkpoint from epoch 1\n",
      "Parameters: 379,547,916\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Compute derived values\n",
    "d_head = config[\"d_model\"] // config[\"n_heads\"]\n",
    "kv_d_head = config[\"d_model\"] // config[\"n_kv_heads\"]\n",
    "\n",
    "model = Llama(\n",
    "    vocab_size=config[\"vocab_size\"],\n",
    "    n_layers=config[\"n_layers\"],\n",
    "    d_model=config[\"d_model\"],\n",
    "    d_head=d_head,\n",
    "    n_heads=config[\"n_heads\"],\n",
    "    n_kv_heads=config[\"n_kv_heads\"],\n",
    "    kv_d_head=kv_d_head,\n",
    "    d_ff_standard=config[\"d_ff_standard\"],\n",
    "    num_experts=config[\"num_experts\"],\n",
    "    num_experts_per_tok=config[\"num_experts_per_tok\"],\n",
    "    d_expert=config[\"d_expert\"],\n",
    "    rope_layers_ratio=config[\"rope_layers_ratio\"],\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    rope_theta=config[\"rope_theta\"],\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = PROJECT_ROOT / \"text_pretraining\" / \"checkpoints\" / \"best.pt\"\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model = model.to(device).eval()\n",
    "\n",
    "print(f\"Loaded checkpoint from epoch {ckpt.get('epoch', 'N/A')}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt, max_tokens=100, temperature=0.8, top_k=50):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt).ids\n",
    "    input_ids = torch.tensor([input_ids], device=device)\n",
    "    \n",
    "    eos_token_id = tokenizer.token_to_id(\"</s>\")\n",
    "    \n",
    "    generated = []\n",
    "    for _ in range(max_tokens):\n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "        next_logits = logits[0, -1, :] / temperature\n",
    "        \n",
    "        # Top-k sampling\n",
    "        if top_k > 0:\n",
    "            indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][-1]\n",
    "            next_logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Sample\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        if next_token == eos_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token)\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if input_ids.shape[1] > config[\"max_seq_len\"]:\n",
    "            input_ids = input_ids[:, -config[\"max_seq_len\"]:]\n",
    "    \n",
    "    return tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROMPT: The capital of France is\n",
      "============================================================\n",
      "The capital of France is the Andes, which means 'the town of Anhalt.'\n",
      "\n",
      "But why does this matter to us? Well, understanding how different cultures come together can help us appreciate our shared history today. For instance, did you know that many different languages have been spoken around the globe since ancient times? That means there are countless languages spoken across the world!\n",
      "\n",
      "So, let's dive into\n",
      "\n",
      "============================================================\n",
      "PROMPT: Machine learning is\n",
      "============================================================\n",
      "Machine learning is a powerful tool that allows us to learn from data and make predictions with new knowledge. In this course unit, we will explore how to train a neural network using the OpenRAP library in Python. We will cover the following topics:\n",
      "\n",
      "1. The `ML` class\n",
      "2. Understanding the `neural_neural_neural_neural_neural_neural_\n",
      "\n",
      "============================================================\n",
      "PROMPT: def fibonacci(n):\n",
      "============================================================\n",
      "def fibonacci(n):\n",
      "    n = len(N)\n",
      "    if len(N) > 1 and len(N) < 1 and len(N) >= 1 and len(N) != 1:\n",
      "        all = False\n",
      "        for i in range(1, N):\n",
      "            if i >= 1:\n",
      "                else:\n",
      "                    all = True\n",
      "                if i == 0:\n",
      "                    all = True\n",
      "                else:\n",
      "                    all =\n",
      "\n",
      "============================================================\n",
      "PROMPT: The solar system\n",
      "============================================================\n",
      "The solar system is a complex system that protects us from the sun. These planets are so tiny that they can't just be seen with our eyes alone. They can be found in everything from the air we breathe to the water we drink to the air we drink.\n",
      "\n",
      "But what's the difference between the two worlds? The solar system is formed by the sun and wind, and the wind is made up of\n",
      "\n",
      "============================================================\n",
      "PROMPT: In the year 2050,\n",
      "============================================================\n",
      "In the year 2050, the Pacific Ocean decided to host a \"The Great Uphillion of the Pacific\" event. This event saw the establishment of a new generation of independent Pacific island nations, and they were excited to start a brand-new island nation.\n",
      "\n",
      "To be honest, one of the island's most well-known island nations was the Pacific Islands. However, this island was not designed to be a\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"Machine learning is\",\n",
    "    \"def fibonacci(n):\",\n",
    "    \"The solar system\",\n",
    "    \"In the year 2050,\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    output = generate(prompt, max_tokens=80, temperature=0.7)\n",
    "    print(f\"{prompt}{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Decoding (Deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy generation:\n",
      "\n",
      "PROMPT: The capital of France is\n",
      "The capital of France is the city of Paris, France, known for its stunning architecture, rich culture, and delicious food. But did you know that there are also some amazing places in France? Today, we will explore one such place called the \"Les Rivier de L'Ouverture,\" which is located in the heart of the city of Paris.\n",
      "\n",
      "Now, let's imagine you are a little girl living in Paris, France. You love playing soccer, going to school, and going to school. But one day, you realize that you don't like your favorite sports team or food. Your parents tell you that you can't play soccer anymore and you can't play with your friends anymore. What would you do?\n",
      "\n",
      "This is where the L'Ouverture comes in. It's a special place for people who love playing soccer, but they don't like their country. They want to make sure that everyone in Paris is happy and healthy. So, they set a\n",
      "\n",
      "PROMPT: Machine learning is\n",
      "Machine learning is a powerful tool for enhancing the performance of machine learning models. By combining the strengths of both approaches, we can create more robust and adaptive machine learning models that can accurately predict and optimize their performance. In the vast and complex landscape of juvenile fiction, there exists a subgenre that combines elements of fantasy, adventure, and adventure - the subgenre of fantasy. This subgenre, known as \"Epic fantasy,\" combines elements of fantasy, adventure, and the supernatural to create engaging narratives that captivate young readers while exploring themes of adventure, discovery, and personal growth. To truly appreciate the significance of these narratives within the broader context of juvenile fiction, it is essential to examine their historical roots, cultural significance, and contemporary relevance.\n",
      "\n",
      "To begin our journey through the realm of juvenile fiction, let us first establish a working definition of what constitutes a fantasy. At its core, fantasy is a subgenre of fantasy that combines elements of fantasy, fantasy, and the supernatural. These narratives often feature magical creatures, supernatural beings, and\n",
      "\n",
      "PROMPT: def fibonacci(n):\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    This function takes a single number as input and returns a single number. This function can be used to perform various operations, such as addition, subtraction, multiplication, and division.\n",
      "\n",
      "The function first checks if the number of elements in the number of elements is less than or equal to the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in the number of elements in\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_greedy(prompt, max_tokens=100):\n",
    "    \"\"\"Greedy decoding - always pick highest probability token.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt).ids\n",
    "    input_ids = torch.tensor([input_ids], device=device)\n",
    "    \n",
    "    eos_token_id = tokenizer.token_to_id(\"</s>\")\n",
    "    \n",
    "    generated = []\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(input_ids)\n",
    "        next_token = logits[0, -1, :].argmax().item()\n",
    "        \n",
    "        if next_token == eos_token_id:\n",
    "            break\n",
    "            \n",
    "        generated.append(next_token)\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "        \n",
    "        if input_ids.shape[1] > config[\"max_seq_len\"]:\n",
    "            input_ids = input_ids[:, -config[\"max_seq_len\"]:]\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "print(\"Greedy generation:\")\n",
    "for prompt in prompts[:3]:\n",
    "    print(f\"\\nPROMPT: {prompt}\")\n",
    "    output = generate_greedy(prompt, max_tokens=200)\n",
    "    print(f\"{prompt}{output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
