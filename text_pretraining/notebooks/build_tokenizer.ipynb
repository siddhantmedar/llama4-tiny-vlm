{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63246be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4516374bd2b04afc9f05dbbc383c45ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9969993a2ad4afb8719d527efa16bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import ByteLevel as ByteLevelProcessor\n",
    "from tokenizers import decoders\n",
    "\n",
    "# Load streaming dataset\n",
    "ds = load_dataset(\n",
    "    \"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c052909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cf900ccf-407d-49d4-bd96-a4679d36c0fc)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus/resolve/3ba9d605774198c5868892d7a8deda78031a781f/cosmopedia-v2/train-00000-of-00104.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6641a4d0-cd6b-4aad-84e5-e933ef17dc59)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus/resolve/3ba9d605774198c5868892d7a8deda78031a781f/cosmopedia-v2/train-00002-of-00104.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a text iterator (adjust column name if needed)\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    batch = []\n",
    "    for sample in dataset:\n",
    "        batch.append(sample[\"text\"])  # Check your column name\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = ByteLevelProcessor(trim_offsets=False)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=[\n",
    "        \"<unk>\",\n",
    "        \"<s>\",  # bos\n",
    "        \"</s>\",  # eos\n",
    "        \"<pad>\",\n",
    "    ],\n",
    "    min_frequency=2,  # skip ultra-rare tokens\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "subset_ds = ds.take(2_000_000)\n",
    "tokenizer.train_from_iterator(batch_iterator(subset_ds), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "tokenizer.save(\"../tokenizer/bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04d2a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Ä world', '!']\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "print(tokenizer.encode(\"Hello, world!\").tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
